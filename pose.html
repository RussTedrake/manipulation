<!DOCTYPE html>

<html>

  <head>
    <title>Robotic Manipulation: Geometric Pose Estimation</title>
    <meta name="Robotic Manipulation: Geometric Pose Estimation" content="text/html; charset=utf-8;" />
    <link rel="canonical" href="http://manipulation.csail.mit.edu/pose.html" />

    <script src="https://hypothes.is/embed.js" async></script>
    <script type="text/javascript" src="htmlbook/book.js"></script>

    <script src="htmlbook/mathjax-config.js" defer></script> 
    <script type="text/javascript" id="MathJax-script" defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    <script>window.MathJax || document.write('<script type="text/javascript" src="htmlbook/MathJax/es5/tex-chtml.js" defer><\/script>')</script>

    <link rel="stylesheet" href="htmlbook/highlight/styles/default.css">
    <script src="htmlbook/highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="stylesheet" type="text/css" href="htmlbook/book.css" />
  </head>

<body onload="loadChapter('manipulation');">

<div data-type="titlepage">
  <header>
    <h1><a href="index.html" style="text-decoration:none;">Robotic Manipulation</a></h1>
    <p data-type="subtitle">Perception, Planning, and Control</p> 
    <p style="font-size: 18px;"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p style="font-size: 14px; text-align: right;"> 
      &copy; Russ Tedrake, 2020<br/>
      Last modified <span id="last_modified"></span>.</br>
      <script>
      var d = new Date(document.lastModified);
      document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      <a href="misc.html">How to cite these notes, use annotations, and give feedback.</a><br/>
    </p>
  </header>
</div>

<p><b>Note:</b> These are working notes used for <a
href="http://manipulation.csail.mit.edu/Fall2020/">a course being taught
at MIT</a>. They will be updated throughout the Fall 2020 semester.  <!-- <a 
href="https://www.youtube.com/channel/UChfUOAhz7ynELF-s_1LPpWg">Lecture  videos are available on YouTube</a>.--></p> 

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=pick.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=deep_perception.html>Next Chapter</a></td>
</tr></table>


<!-- EVERYTHING ABOVE THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->
<chapter style="counter-reset: chapter 3"><h1>Geometric Pose Estimation</h1>
  <a style="float:right; margin-top:-80px;" target="pose" href="https://colab.research.google.com/github/RussTedrake/manipulation/blob/master/pose.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open Corresponding Notebook In Colab"/></a>
  <div style="clear:right;"></div>

  <p>In the last chapter, we developed an initial solution to moving objects
  around, but we made one major assumption that would prevent us from using it
  on a real robot: we assumed that we knew the initial pose of the object.  This
  chapter is going to be our first pass at removing that assumption, by
  developing tools to estimate that pose using the information obtained from the
  robot's depth cameras.  The tools we develop here will be most useful when you
  are trying to manipulate <i>known objects</i> (e.g. you have a mesh file of
  their geometry) and are in a relatively <i>uncluttered</i> environment.  But
  they will also form a baseline for the more sophisticated methods we will
  study.</p>

  <p>The approach that we'll take here is very geometric.  This is in contrast
  to, and very complimentary with, approached that are more fundamentally driven
  by data.  There is no question that deep learning has had an enormous impact
  in perception -- it's fair to say that it has enabling the current surge in
  manipulation advances -- and we will certainly cover it in these notes.  But
  when I've heard colleagues say that "all perception these days is based on
  deep learning", I can't help but cry foul.  There has been another surge of
  progress in the last few years that has been happening in parallel: the
  revolution in geometric reasoning, fueled by applications in autonomous
  driving and virtual/augmented reality in addition to manipulation.  Most
  advanced manipulation systems these days combine both "deep perception" and
  "geometric perception".</p>

  <section><h1>Cameras and depth sensors</h1>
  
    <p>Just as we had many choices when selecting the robot arm and hand, we
    have many choices for instrumenting our robot/environment with sensors. Even
    more so than our robot arms, the last few years have seen incredible
    improvements in the quality and reductions in cost and size for these
    sensors.  This is largely thanks to the cell phone industry, but the race
    for autonomous cars has been fueling high-end sensors as well.</p>

    <p>These changes in hardware quality have causes sometimes dramatic changes
    in our algorithmic approaches.  For example, estimation can be much easier
    when the resolution and frame rate of our sensors is high enough that not
    much can change in the world between two images; this undoubtedly
    contributed to the revolutions in the field of "simultaneous localization
    and mapping" (SLAM) we have seen over the last decade or so.</p>

    <p>One might think that the most important sensors for manipulation are the
    touch sensors (you might even be right!).  But in practice today, most of
    the emphasis is on camera-based and/or range sensing.  At very least, we
    should consider this first, since our touch sensors won't do us much good if
    we don't know where in the world we need to touch.</p>

    <p>Traditional cameras, which we think of as a sensor that outputs a color
    image at some framerate, play an important role.  But robotics makes heavy
    use of sensors that make an explicit measurement of the distance (between
    the camera and the world) or depth; sometimes in addition to color and
    sometimes in lieu of color.  Admittedly, some researchers think we should
    only rely on color images.</p>

    <subsection><h1>Depth sensors</h1>
    
      <p>Primarily RGB-D (ToF vs projected texture stereo vs ...) cameras and Lidar</p>

      <p>The cameras we are using in this course are <a href="https://software.intel.com/en-us/realsense/d400">Intel RealSense D415</a>....</p>

      <p>Monocular depth.</p>

      <todo>How the kinect works in 2 minutes: https://www.youtube.com/watch?v=uq9SEJxZiUg</todo>

    </subsection>

    <subsection><h1>Simulation</h1>
    
      <p>There are a number of levels of fidelity at which one can simulate a
      camera like the D415.  We'll start our discussion here using an "ideal"
      RGB-D camera simulation -- the pixels returned in the depth image
      represent the true geometric depth in the direction of each pixel
      coordinate.  In <drake></drake> that is represented by the
      <code>RgbdSensor</code> system, which can be wired up directly to the
      <code>SceneGraph</code>.</p>

      <div>
        <script src="htmlbook/js-yaml.min.js"></script>
        <script type="text/javascript">
        var sys = jsyaml.load(`
name: RgbdSensor
input_ports:
- geometry_query
output_ports:
- color_image
- depth_image_32f
- depth_image_16u
- label_image
- X_WB`);
        document.write(system_html(sys, "https://drake.mit.edu/doxygen_cxx/classdrake_1_1systems_1_1sensors_1_1_rgbd_sensor.html"));
        </script>
      </div>

      <p>The signals and systems abstraction here is encapsulating a lot of
      complexity.  Inside the implementation of that system is a complete
      rendering engine, like one that you will find in high-end computer games.
      Drake actually support multiple rendering engines; for the purposes of
      this class we will primarily use an OpenGL-based renderer that is suitable
      for real-time simulation.  In our research we also use Drake's rendering
      API to connect to high-end <a
      href="https://en.wikipedia.org/wiki/Physically_based_rendering">physically-based
      rendering (PBR)</a> based on ray tracing, such as the <a
      href="https://www.blender.org/features/rendering/">Cycles renderer
      provided by Blender</a>.  These are most useful when we are trying to
      render higher quality images e.g. to <i>train</i> a deep perception
      system; we will discuss them in the deep perception chapters.</p>

      <example class="drake"><h1>Simulating an RGB-D camera</h1>

        <p><a target="pose"
          href="https://colab.research.google.com/github/RussTedrake/manipulation/blob/master/pose.ipynb#scrollTo=7q0A14bAilIX"><img
          src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
          </p>

        <p>As a simple example of depth cameras in drake, I've constructed a
        scene with a single object (the mustard bottle from the <a
        href="https://github.com/RobotLocomotion/models/tree/master/ycb">YCB
        dataset</a>), and added an <code>RgbdSensor</code> to the diagram.  Once
        this is wired up, we can simply evaluate the output ports in order to
        obtain the color and depth images:</p>

        <figure><img src="data/mustard.svg"/></figure>
    
        <p><img style="height:200px;float:right;margin-left:10px"
        src="data/mustard_meshcat.png"/> Please make sure you spend a minute
        with the MeshCat visualization (<a href="data/mustard.html">available
        here</a>).  You'll see a camera and the camera frame, and you'll see the
        mustard bottle as always... but it might look a little funny.  That's
        because I'm displaying both the ground truth mustard bottle model
        <i>and</i> the point cloud rendered from the cameras.  You can use the
        MeshCat GUI to uncheck the ground truth visualization (image right), and you'll be able to see just the point cloud.</p>

        <p>Remember that, in addition to looking at the source code if you like,
        you can always inspect the block diagram to understand what is happening
        at the "systems level".  <a href="data/depth_camera_diagram.svg">Here is the diagram</a> used in this example.</p>

      </example>
    
      <p>In the <code>ManipulationStation</code> simulation of the entire robot
      system for class, the <code>RgbdSensors</code> have already been added,
      and their output ports exposed as outputs for the station.</p>

      <p>Real depth sensors, of course, are far from ideal -- and errors in
      depth returns are not simple Gaussian noise, but rather are dependent on
      the lighting conditions, the surface normals of the object, and the visual
      material properties of the object, among other things.  The color channels
      are an approximation, too.  Even our high-end renderers can only do as
      well as the geometries, materials and lighting sources that we add to our
      scene, and it is very hard to capture all of the nuances of a real
      environment.  We will examine real sensor data, and the gaps between
      modeled sensors and real sensors, after we start understanding some of the
      basic operations.</p>

      <figure><img style="width:95%" src="data/Sweeney18a_fig3.png"/><figcaption>Real depth
      sensors are messy.  The red pixels in the far right image are missed
      returns -- the depth sensor returned "maximum depth".  Missed returns,
      especially on the edges of objects (or on reflective objects) are a common
      sighting in raw depth data.  This figure is reproduced from
      <elib>Sweeney18a</elib>.</figcaption></figure>
    </subsection>

    <subsection><h1>Occlusions and partial views</h1>
    
      <p>The view of the mustard bottle in the example above makes one primary
      challenge of working with cameras very clear. Cameras don't get to see
      everything!  They only get line of sight.  If you want to get points from
      the back of the mustard bottle, you'll need to move the camera.  And if
      the mustard bottle is sitting on the table, you'll have to pick it up if
      you ever want to see the bottom.  This situation gets even worse in
      cluttered scenes, where we have our views of the bottle <i>occluded</i> by
      other objects.  Even when the scene is not cluttered, a head-mounted or
      table-mounted camera is often blocked by the robot's hand when it goes to
      do manipulation -- the moments when you would like the sensors to be at
      their best is often when they give no information!</p>

      <p>It is quite common to mount a depth camera on the robot's wrist in
      order to help with the partial views.  But it doesn't completely resolve
      the problem.  All of our depth sensors have both a maximum range and a
      minimum range.  The RealSense D415 returns matches from about 0.3m to
      1.5m.  That means for the last 30cm of the approach to the object --
      again, where we might like our cameras to be at their best -- the object
      effectively disappears!</p>

      <figure><img style="width:80%" src="data/dishloading_cameras.svg">
      <figcaption>When luxury permits, we try to put as many cameras as we can
      into the robot's environment.  In this dish-loading testbed at <a
      href="https://www.tri.global/news/tri-taking-on-the-hard-problems-in-manipulation-re-2019-6-27">TRI</a>
      where we put a total of eight cameras into the scene (it still doesn't
      resolve the occlusion problem).</figcaption></figure>

    </subsection>
  </section>

  <section><h1>Representations for geometry</h1>

    <p>Depth image, point cloud, triangulated mesh, signed distance functions, (+ surfels, occupancy maps, ...)</p>

    <p>The data returned by a depth camera takes the form of an image, where
    each pixel value is a single number that represents the distance between the
    camera and the nearest object in the environment along the pixel direction.
    If we combine this with the basic information about the camera's intrinsic
    parameters (e.g. lens parameters, stored in the <a
    href="https://drake.mit.edu/doxygen_cxx/classdrake_1_1systems_1_1sensors_1_1_camera_info.html"><code>CameraInfo</code></a>
    class in Drake) then we can transform this <i>depth image representation</i>
    into a collection of 3D points, $s_i$.  I use $s$ here because they are
    commonly referred to as the "scene points" in the algorithms we will present
    below.  These points have a pose and (optionally) a color value or other
    information attached.  This is known as a <i>point cloud representation</i>.
    By default, their pose is described in the camera frame, $^CX^{s_i}$, but
    the <code>DepthImageToPointCloud</code> system in Drake also accepts $^PX^C$
    to make it easy to transform them into another frame (such as the world
    frame).</p>  
    
    <div>
      <script src="htmlbook/js-yaml.min.js"></script>
      <script type="text/javascript">
      var sys = jsyaml.load(`
name: DepthImageToPointCloud
input_ports:
- depth_image
- color_image (optional)
- camera_pose (optional)
output_ports:
- point_cloud`);
      document.write(system_html(sys, "http://drake.mit.edu/doxygen_cxx/classdrake_1_1perception_1_1_depth_image_to_point_cloud.html"));
      </script>
    </div>
    
    <p>As depth sensors have become so pervasive the field has built up
    libraries of tools for performing basic goemetric operations operate on
    point clouds, and that can be used to transform back and forth between
    representations.  We can implement many of of the basic operations directly
    in Drake, but I would recommend the <a
    href="http://www.open3d.org/">Open3D</a> library if you need more (I have an
    example of transferring data from the Drake image and point cloud
    representations into Open3D <a href="manipulation/open3d/">here</a>).  Many
    older projects used the <a href="http://pointclouds.org/">Point Cloud
    Library (PCL)</a>, which is now defunct but still has some very useful
    documentation.</p>

    <p>It's important to realize that the conversion of a depth image into a
    point cloud does loose some information -- specifically the information
    about the ray that was cast from the camera to arrive at that point.  In
    addition to declaring "there is geometry at this point", the depth image
    combined with the camera pose also implies that "there is no geometry in the
    straight line path between camera and this point".  We will make use of this
    information in some of our algorithms, so don't discard the depth images
    completely!  More generally, we will find that each of the different
    geometry representations have strengths and weaknesses -- it is very common
    to keep multiple representations around and to convert back and forth
    between them.</p>

  </section>

  <!--
  <section><h1>Working with point clouds</h1>

    <p>First, let us get a few simulated point clouds to work with.  We can use either <drake></drake> or Open3d to convert the depth image into a point cloud; we'll use Drake for this step here simply because we already have the camera parameters in the Drake format.  Although we can call the conversion functions directly, we can also add a system that does the conversion for us in the systems framework:</p>

    <table align="center" cellpadding="0" cellspacing="0">
      <tr align="center">
      <td><table cellspacing="0" cellpadding="0">
      <tr>
      <td align="right" style="padding:5px 0px 5px 0px">depth_image &rarr; </td></tr>
      <tr>
      <td align="right" style="padding:5px 0px 5px 0px">color_image (optional) &rarr; </td></tr>
      <tr>
      <td align="right" style="padding:5px 0px 5px 0px">camera_pose (optional) &rarr;</td></tr>
      </table>
      </td><td align="center" style="border:solid;padding-left:20px;padding-right:20px" bgcolor="#F0F0F0"><a class="el" href="http://drake.mit.edu/doxygen_cxx/classdrake_1_1perception_1_1_depth_image_to_point_cloud.html" title="Converts a depth image to a point cloud.">DepthImageToPointCloud</a></td><td><table cellspacing="0" cellpadding="0">
      <tr>
      <td align="left" style="padding:5px 0px 5px 0px">&rarr; point_cloud </td></tr>
      </table>
      </td></tr>
    </table>

    <p>You can infer from the optional input ports on this system that point cloud representations can potentially include color values for each of the Cartesian points.  The following example will get us a few point clouds to work with.</p>

    <todo>Combine all point clouds via https://drake.mit.edu/pydrake/pydrake.systems.perception.html?highlight=pointcloud#pydrake.systems.perception.PointCloudConcatenation</todo>

    <todo>Show the point cloud in Open3D's visualizer</todo>

    There are a number of basic operations that one can do on point clouds.  I'll give just a few examples here.  

    <subsection><h1>Surface normal estimation</h1>
    
      <p>The simplest and most straight-forward algorithm for estimating the normals of a point cloud surface is to find the $k$ nearest neighbors, and fit a plane through those points using least-squares.  Let's denote the nominal point (about which we are estimating the normal) $p_0$, and it's $k$ nearest neighboard $p_i, i\in[1,k].$  Let us form the $k \times 3$ data matrix: $$X = [ p_1 - p_0, p_2 - p_0, ..., p_k - p_0 ].$$  Then the principal components of this data matrix are given by the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> of $X$ (or the eigenvectors of $X^T X$); if $X = U\Sigma V^T$, then the columns of $V$ are the principal directions, with the last column (associated with the smallest singular value) representing the normal direction.  The length can be normalized to one; but keep in mind that the sign of this vector is ambiguous -- the $k$ nearest points don't give any information about what direction might be into the object or out of the object.  This is a case where knowing the original camera direction can help -- for example, PCL offers a method <code>flipNormalTowardsViewpoint</code> that post-processes the normals.</p>

      <figure>
        <img src="http://www.pointclouds.org/assets/images/contents/documentation/features_normal.png" />

        <figcaption>Normal estimation using $k$ nearest neighbords (image linked from the <a href="http://docs.pointclouds.org/trunk/group__features.html"></a>PCL documentation</a>)</figcaption>
      </figure>

      <todo>Add an example; or even support normals in the DepthImageToPointCloud.  Example could be as simple as grabbing one point from the mustard point cloud, and finding it's normal (with everything plotted in matplotlib).</todo>

      <p>What was interesting and surprising (to me) about this is not that the least-squares solution works.  What is surprising is that this operation is considered commonplace and cheap -- for every point in the point cloud, we find $k$ nearest neighbors and do the normal estimation... even for very large point clouds.  Making this performant typically requires data structures like the <a href="https://en.wikipedia.org/wiki/K-d_tree">k-d tree</a> to make the nearest neighbor queries efficient.</p>

      <p>Another standard feature that can be extracted from the $k$ nearest neighbors is the local curvature.  The math is quite similar.  <todo>add it here</todo></p>

    </subsection>

    <subsection><h1>Plane segmentation</h1>
    
      <p>Moving beyond local properties like surface normals and local curvature, we might choose to process a point cloud into primitive shapes -- like boxes or spheres.  Another practical and natural segmentation is segmenting surfaces into planes... which we can accomplish with heuristic algorithms which group neighboring points with similar surface normals.  We used this effectively in the DARPA Robotics Challenge.</p>

      <todo>Insert DRC plane segmentation videos here, from https://drive.google.com/drive/folders/1gYMJ0djBXbevWDBpekkK58pcbtZFr0A0 </todo>

    </subsection>

  </section>
  -->

  <section><h1>Point cloud registration with known correspondences</h1>
    
    <p>Let us begin to address the primary objective of the chapter -- we have a
    known object somewhere in the robot's workspace, we've obtained a point
    cloud from our depth cameras.  How do we estimate the pose of the object,
    $X^O$?</p>

    <p><img style="height:90px;float:left;margin-right:10px"
    src="data/icp_model_points_sketch.png"/>One very natural and well-studied
    formulation of this problem comes from the literature on <a
    href="https://en.wikipedia.org/wiki/Point_set_registration">point cloud
    registration</a> (also known as point set registration or scan matching).
    Given two point clouds, point cloud registration aims to find a rigid
    transform that optimally aligns the two point clouds.  For our purposes,
    that suggests that our "model" for the object should also take the form of a
    point cloud (at least for now).  We'll describe that object with a list of
    <i>model points</i>, $m_i$, with their pose described in the object frame:
    $^OX^{m_i}.$ </p>

    <p>Our second point cloud will be the <i>scene points</i>, $s_i$, that we
    obtain from our depth camera, transformed (via the camera intrinsics) into
    the camera coordinates, $^CX^{s_i}$.  I'll use $N_m$ and $N_s$ for the
    number of model and scene points, respectively.</p>  
      
    <p>Let us assume, for now, that we also know $X^C.$  Is this reasonable? In
    the case of a wrist-mounted camera, this would amount to solving the forward
    kinematics of the arm.  In an environment-mounted camera, this is about
    knowing the pose of the cameras in the world.  Small errors in estimating
    those poses, though, can lead to large artifacts in the estimated point
    clouds.  We therefore take great care to perform <a
    href="station.html">camera extrinsics calibration</a>; I've linked to our
    calibration code in the <a href="station.html">appendix</a>.  Note that if
    you have a mobile manipulation platform (an arm mounted on a moving base),
    then all of these discussions still apply, but you likely will perform all
    of your registration in the robot's frame instead of the world frame.</p>

    <p>The second <b>major assumption</b> that we will make in this section is
    "known correspondences".  This is <i>not</i> a reasonable assumption in
    practice, but it helps us get started.  The assumption is that we know which
    scene point corresponds with which model point.  I'll use a correspondence
    vector $c \in [1,N_m]^{N_s}$, where $c_i = j$ denotes that scene point $s_i$
    corresponds with model point $m_j$.  Note that we do not assume that these
    correspondence are one-to-one.  Every scene point corresponds to some model
    point, but the converse is not true, and multiple scene points may
    correspond to the same model point.</p>

    <p>As a result, the point cloud registration problem is simply an (inverse)
    kinematics problem.  We can write the model points and the scene points in a
    common frame (here the world frame), $$p^{m_{c_i}} = X^O {^Op^{m_{c_i}}} =
    X^C {^Cp^{s_i}},$$leaving a single unknown transform, $X^O$, for which we
    must solve.  In the previous chapter I argued for using differential
    kinematics instead of inverse kinematics; why is my story different here?
    Differential kinematics can still be useful for perception, for example if
    we want to track the pose of an object after we acquire it's initial pose.
    But unlike the robot case, where we can read the joint sensors to get our
    current state, in perception we need to solve this harder problem at least
    once.</p>

    <p>What does a solution for $X^O$ look like?  Each model point gives us
    three constraints, when $p^{s_i} \in \Re^3.$  The exact number of unknowns
    depends on the particular representation we choose for the pose, but almost
    always this problem is dramatically over-constrained.  Treating each point
    as hard constraints on the relative pose would also be very susceptible to
    measurement noise.  As a result, it will serve us better to try to find a
    pose that describes the data, e.g., in a least-squares sense: $$\min_{X^O}
    \sum_{i=1}^{N_s} \| X^O {^Op^{m_{c_i}}} - X^C {^Cp^{s_i}}\|^2.$$</p>

    <p>To proceed, let's pick a particular representation for the pose to work
    with.  I will use 3x3 rotation matrices here; the approach I'll describe
    below also has an equivalent in quaternion form<elib>Horn87</elib>.  To
    write the optimization above using the coefficients of the rotation matrix
    and the translation as decision variables, I have: \begin{align} \min_{p^O,
    R^O} \quad& \sum_{i=1}^{N_s} \| p^O + R^O{^Op^{m_{c_i}}} - X^C
    {^Cp^{s_i}}\|^2, \\ \subjto \quad& R^T = R^{-1}, \quad \det(R) = +1.
    \nonumber \end{align}  The constraints are needed because not every 3x3
    matrix is a valid rotation matrix; satisfying these constraints ensures that
    $R$ <i>is</i> a valid rotation matrix.  In fact, the constraint $R^T =
    R^{-1}$ is almost enough by itself; it implies that $\det(R) = \pm 1.$ But a
    matrix that satisfies that constraint with $\det(R) = -1$ is called an "<a
    href="https://en.wikipedia.org/wiki/Improper_rotation">improper
    rotation</a>", or a rotation plus a reflection across the axis of
    rotation.</p>

    <p>Let's think about the type of optimization problem we've formulated so
    far.  Given our decision to use rotation matrices, the term $p^O +
    R^O{^Op^{m_{c_i}}} - X^C {^Cp^{s_i}}$ is linear in the decision variables
    (think $Ax \approx b$), making the objective a convex quadratic.  How about
    the constraints?  The first constraint is a quadratic equality constraint
    (to see it, rewrite it as $RR^T=I$ which gives 9 constraints, each quadratic
    in the elements of $R$).  The determinant constraint is cubic in the
    elements of $R$.</p>

    <example><h1>Rotation-only point registration in 2D</h1>
        
      <p>I would like you to have a graphical sense for this optimization
      problem, but it's hard to plot the objective function with 9+3 decision
      variables.  To whittle it down to the essentials that I can plot, let's
      consider the case where the scene points are known to differ from the
      model points by a rotation only (this is famously known as <a
      href="https://en.wikipedia.org/wiki/Wahba%27s_problem">the Wahba
      problem</a>).  To further simplify, let's consider the problem in 2D
      instead of 3D.</p>

      <p>In 2D, we can actually linearly parameterize a rotation matrix with
      just two variables:  $$R = \begin{bmatrix} a & -b \\ b & a\end{bmatrix}.$$
      With this parameterization, the constraints $RR^T=I$ and the determinant
      constraints are identical; they both require that $a^2 + b^2 = 1.$</p>

      <figure><img style="width:80%"
      src="data/wahba2d.png"/></figure><figcaption>The $x$ and $y$ axes of this
      plot correspond to the parameters $a$, and $b$, respectively, that define
      the 2D rotation matrix, $R$.  The green quadratic bowl is the objective
      $\sum_i \| R {p^{m_{c_i}}} - p^{s_i}\|^2,$ and the red open cylinder is
      the rotation matrix constraint.  <a
      href="https://www.geogebra.org/3d/pn628maw">Click here for the interactive
      version.</a></figcaption>

      <p>Is that what you expected?  I generated this plot using just two
      model/scene points, but adding more will only change the shape of the
      quadratic, but not it's minimum.  And on the <a
      href="https://www.geogebra.org/3d/pn628maw">interactive version</a> of the
      plot, I've added a slider so that you control the parameter, $\theta$,
      that represents the ground truth rotation described by $R$. Try it
      out!</p>

      <p>In the case with no noise in the measurements (e.g. the scene points
      are exactly the model points modified by a rotation matrix), then the
      minimum of the objective function already satisfies the constraint.  But
      if you add just a little noise to the points, then this is no longer true,
      and the constraint starts to play an important role.</p>
    </example>

    <p>The geometry should be roughly the same in 3D, though clearly in much
    higher dimensions.  But I hope that the plot makes it perhaps a little less
    surprising that this problem has an elegant numerical solution based on the
    singular value decomposition (SVD).</p>
    
    <p>What about translations?  There is a super important insight that allows
    us to decouple the optimization of rotations from the optimization of
    translations.  The insight is this: <i>the <b>relative</b> position between
    points is effected by rotation, but not by translation</i>. Therefore, if we
    write the points relative to some canonical point on the object, we can
    solve for rotations alone.  Once we have the rotation, then we can back out
    the translations easily.  For our least squares objective, there is even a
    "correct" canonical point to use -- the <i>central point</i> (like the
    center of mass if all points have equal mass) under the Euclidean
    metric.</p>

    <p>Therefore, to obtain the solution to the problem, \begin{align}
    \min_{p\in\Re^3,R\in\Re^{3\times3}} \quad & \sum_{i=1}^{N_s} \| p + R
    \hspace{.1cm} {^Op^{m_{c_i}}} - p^{s_i}\|^2, \\ \subjto \quad & RR^T =
    I,\end{align} first we define the central model and scene points, $\bar{m}$
    and $\bar{s}$, with positions given by $$^Op^\bar{m} = \frac{1}{N_s}
    \sum_{i=1}^{N_s} {^Op^{m_{c_i}}}, \qquad p^\bar{s} = \frac{1}{N_s}
    \sum_{i=1}^{N_s} p^{s_i}.$$  If you are curious, these come from taking the
    gradient of the <a
    href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrangian</a>
    with respect to $p$, and setting it equal to zero: $$\sum_{i=1}^{N_s} 2 (p +
    R \hspace{.1cm} {^Op^{m_{c_i}}} - p^{s_i}) = 0 \Rightarrow p^* =
    \frac{1}{N_s} \sum_{i=1}^{N_s} {^Op^{s_i}} - R^*\left(\frac{1}{N_s}
    \sum_{i=1}^{N_s} {^Op^{m_{c_i}}}\right),$$ but don't forget the geometric
    interpretation above.  This is just another way to see that we can
    substitute the right-hand side in for $p$ in our objective and solve for $R$
    by itself.</p> 
      
    <p>To find $R$, compose the data matrix $$W = \sum_{i=1}^{N_s} (p^{s_i} -
    p^\bar{s}) (^Op^{m_{c_i}} - {^Op^\bar{m}})^T, $$ and use SVD to compute $W =
    U \Sigma V^T$.  The optimal solution is \begin{gather*} R^* = U D V^T,\\ p^*
    = {^Op^\bar{s}} - R^* p^\bar{m},\end{gather*} where $D$ is the diagonal
    matrix with entries $[1, 1, \det(UV^T)]$ <elib>Myronenko09</elib>.  This may
    seem like magic, but replacing the singular values in SVD with ones gives
    the optimal projection back onto the orthonormal matrices, and the diagonal
    matrix ensures that we do not get an improper rotation. There many
    derivations available in the literature, but many of them drop the
    determinant constraint instead of handling the improper rotation. See
    <elib>Haralick89</elib> (section 3) for one of my early favorites.</p>  
      
    <example class="drake"><h1>Point cloud registration with known
    correspondences</h1>

      <figure><img style="width:70%; margin-top:-20px; margin-bottom:-20px"
      src="data/pose_from_known_correspondences.svg"/> </figure>

      <p>In the example code, I've made a random object (based on a random set
      of points in the $x$-$y$ plane), and perturbed it by a random 2D
      transform.  The blue points are the model points in the model coordinates,
      and the red points are the scene points.  The green dashed lines represent
      the (perfect) correspondences.  On the right, I've plotted both points
      again, but this time using the estimated pose to put them both in the
      world frame.  As expected, the algorithm perfectly recovers the ground
      truth transformation.</p>

      <p><a target="pose"
      href="https://colab.research.google.com/github/RussTedrake/manipulation/blob/master/pose.ipynb#scrollTo=9QDyRMCyb_gL"><img
      src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open
      In Colab"/></a>
          </p>

    </example>

    <todo>Add exercise on the reflection case.</todo>

    <todo>Confirm or disprove: i think that this is just the least squares
    solution to the unconstrained problem, then projected back to the
    orthonormal matrices (with the SVD).</todo>

    <p>What is important to understand is that once the correspondences are
    given we have an efficient and robust numerical solution to estimating the
    pose.</p>

  </section>

  <section><h1>Iterative Closest Point (ICP)</h1>

    <p>So how do we get the correspondences?  In fact, if we were given the pose
    of the object, then figuring out the correspondences is actually easy: the
    corresponding point on the model is just the nearest neighbor / closest
    point to the scene point when they are transformed into a common frame.</p>

    <p>This observation suggests a natural iterative scheme, where we start with
    some initial guess of the object pose and compute the correspondences via
    closest points, then use those correspondences to update the estimated pose.
    This is one of the famous and often used (despite its well-known
    limitations) algorithms for point cloud registration: the iterative closest
    point (ICP) algorithm.</p>

    <p>To be precise, let's use $\hat{X}^O$ to denote our estimate of the object
    pose, and $\hat{c}$ to denote our estimated correspondences.  The
    "closest-point" step is given by $$\forall i, \hat{c}_i = \argmin_{j \in
    N_m}\| \hat{X}^O {^Op^{m_{c_i}}} - p^{s_i} \|^2.$$  In words, we want to
    find the point in the model that is the closest in Euclidean distance to the
    transformed scene points.  This is the famous "nearest neighbor" problem,
    and we have good numerical solutions (using optimized data structures) for
    it.  For instance, Open3D uses <a
    href="https://github.com/mariusmuja/flann">FLANN</a><elib>Muja09</elib>.</p>

    <p>Although solving for the pose and the correspondences jointly is very
    difficult, ICP leverages the idea if we solve for them independently, then
    both parts have good solutions.  Iterative algorithms like this are a common
    approach taken in optimization for e.g. bilinear optimization or expectation
    maximization.  It is important to understand that this is a local solution
    to a non-convex optimization problem.  So it is subject to getting stuck in
    local minima.</p>

    <example class="drake"><h1>Iterative Closest Point</h1>
  
      <figure>
        <iframe style="border:0;height:300px;width:400px;" src="data/iterative_closest_point.html"></iframe>
      </figure>
  
      <p>Here is ICP running on the random 2D objects.  Blue are the model
      points, red are the scene points, green dashed lines are the
      correspondences.  I encourage you to run the code yourself.</p>  
            
      <p>I've included one of the animation where it happens to converge to the
      true optimal solution.  But it gets stuck in a local minima more often
      than not!  I hope that stepping through will help your intuition.
      Remember that once the correspondences are correct, the pose estimation
      will recover the exact solution.  So every one of those steps before
      convergence has at least a few bad correspondences.  Look closely!</p>
  
      <p><a target="pose"
      href="https://colab.research.google.com/github/RussTedrake/manipulation/blob/master/pose.ipynb#scrollTo=AHfxMwrvb1mz"><img
      src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open
      In Colab"/></a></p>
  
    </example>      
      
    <!--<todo>Thanks to the geometric nature of the problem, many of these local
      minima are graphical and intuitive.  (Will insert a few examples
      here).</todo> -->

    <p>Intuition about these local minima has motivated a number of ICP
    variants, include point to plane ICP, normal ICP, ICP that use color
    information, feature-based ICP, etc.</p>

 <!---     
      <p>Maybe more fundamental, though, is the fact that I don't think the ICP objective, even if we could optimize it perfectly is quite right.  (TODO: add the example of a long thin book).  And there is information that we have, which should aid in estimating the pose, which is not present in the point cloud (like the location of the cameras).</p>
-->
    <todo>Add a simple example and a homework problem/exercise.</todo>

  </section>

  <!--
  <section><h1>Pairwise distance</h1>
    
  </section>
-->

  <section><h1>Dealing with partial views and outliers</h1>

    <p>The example above is sufficient to demonstrate the problems that ICP can
    have with local minima.  But we have so far been dealing with unreasonably
    perfect point clouds.  <!-- call out to point cloud segmentation.
    <elib>Nguyen13</elib> --></p>


    <todo>An example showing how large an effect the outliers can have.</todo>

    <p>There are a variety of ways that one might mitigate the effect of
    outliers in ICP.  The simplest strategy is to simply impose a maximum
    distance in correspondence step -- if the nearest model point is too far
    away to a particular scene point, then don't include it in the pose
    estimation step.</p>

    <subsection><h1>Random sample consensus (RANSAC)</h1>
    
    </subsection>

    <subsection><h1>Point cloud segmentation</h1>
    
      <todo> normal estimation, plane fitting, etc.</todo>

    </subsection>

    <subsection><h1>Generalizing correspondence</h1>
    
      <todo>  Model on scene?  Scene on model?  Pic of the dish robot picking a mug
      in the cluttered sink?</todo>
    
    </subsection>

    <subsection><h1>Soft correspondences</h1>

      <todo>Coherent Point Drift (CPD), e.g. equation 7.</todo>
  
      <todo>FilterReg</todo>
  
    </subsection>
  
    <subsection><h1>Truncated least-squares</h1>

      <p>The real dream is to get rid of the alternations, and solve for the
      correspondences and pose simultaneously.</p>

      <todo>Greg's work.  Teaser.</todo>

      <todo>Truncated least-squares plot.</todo>

      <todo>Mention global optimizers (GoICP, Super4PCS) here?</todo>

    </subsection>
    
  </section>

  <section><h1>Non-penetration and "free-space" constraints</h1>
  
    <p>Dense articulated real-time tracking (DART).</p>
  
  </section>
  
  <section><h1>Looking ahead</h1>
  
    <p>Identifying correspondences between point clouds has been a major theme
    in this chapter -- and we haven't completely solved it.  This is one of the
    many places where deep learning has a lot to offer.  We will discuss
    deep learning approaches for (dense) correspondences (e.g.
    <elib>Florence18a</elib>) soon!</p>

  </section>
  
  <section id="exercises"><h1>Exercises</h1>
  
    <exercise><h1>How many points do you need?</h1>
    
      <p>Consider the problem of point cloud registration with known
      correspondences.  I said that, in most cases, we have far more points than
      we have decision variables, and therefore treating each as an equality
      constraint would make the problem over-constrained.  That raises a natural
      question: what is the minimal number of points required to uniquely
      specify a pose?  In 2D?  In 3D?  (The answer might not be what you might
      first think!)</p>

      <!-- 2 points is sufficient in 2D.  3 in 3D.  The disconnect to our
      intuition (e.g. how many points required to specify a plane), is because
      these points have unique IDs from the correspondences.  And reflections
      aren't allowed.  -->
    
    </exercise>

    <exercise><h1>Rotational symmetries</h1>

      <p>We have a beautiful formulation for the point cloud registration
      problem with known correspondences -- it has a quadratic objective and
      (with the determinant constraint relaxed) a quadratic equality constraint.
      Sometimes we have objects that are rotationally symmetric -- think of box
      that is a perfect cube.  How can the quadratic objective capture a problem
      where there should be equally good solutions at rotations that are 90
      degrees apart?</p>

      <!-- Answer:  It can't!  And this can't happen in our formulation, because
      there is no such thing as rotational symmetry when we've assumed known and
      one-to-one correspondences. -->

    </exercise>

  </section>

</chapter>
<!-- EVERYTHING BELOW THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=pick.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=deep_perception.html>Next Chapter</a></td>
</tr></table>

<div id="footer">
  <hr>
  <table style="width:100%;">
    <tr><td><em>Robotic Manipulation</em></td><td style="text-align:right">&copy; Russ
      Tedrake, 2020</td></tr>
  </table>
</div>


</body>
</html>
