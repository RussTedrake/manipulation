{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRZ5_28xraxv"
   },
   "source": [
    "# Introduction to Segment Anything Model (SAM)\n",
    "\n",
    "The **Segment Anything Model (SAM)** is a powerful **promptable segmentation system**.  \n",
    "Its main strength lies in its ability to **segment objects in any image or video**.\n",
    "\n",
    "Unlike traditional image segmentation models (which are trained for specific object categories like \u201cperson,\u201d \u201ccat,\u201d or \u201ccar\u201d), SAM is a **general-purpose model**.  \n",
    "It can generate object masks based on **different types of prompts**, including:\n",
    "- Points (e.g., \"for this pixel, what is the segmentation for this object\"),\n",
    "- Bounding boxes (e.g., \"segment whatever is inside this box\"),\n",
    "- Or a full image\n",
    "\n",
    "---\n",
    "\n",
    "## Why SAM Matters in Robotics\n",
    "\n",
    "SAM\u2019s generalization power enables robots to **see** their environment. Robots need precise visual perception to safely interact with their surroundings.\n",
    "\n",
    "\n",
    "While SAM is great at segmentation, it **cannot locate specific objects**. It can\u2019t tell you which mask belongs to a \u201cphone\u201d or a \u201ccup.\u201d  \n",
    "To build robots that follow commands like:\n",
    "> \u201cPick up the phone\u201d  \n",
    "we need models that can **visually locate an object with language** \u2014 these are called **Vision-Language Models (VLMs)**.\n",
    "\n",
    "A **VLM** interprets an English command, locates the corresponding object in an image, and returns a **bounding box** around that object.  \n",
    "Then, we can use **SAM**, guided by this bounding box, to produce a **precise segmentation mask** of the object.\n",
    "\n",
    "*Note: Noteably, Grounded SAM can segment objects based on human prompts and return the segmentation mask with a semantic label! Along with VLMs, more general object detection models (like YOLO) can also be used to produce bounding boxes (and can be more efficient/accurate, making them better for certain settings). We won't be exploring these models in this notebook, but feel free to check them out on your own time!*\n",
    "\n",
    "---\n",
    "\n",
    "# Notebook Objectives\n",
    "\n",
    "In this notebook, we\u2019ll explore how SAM and a VLM can work **together** to achieve language-driven segmentation.\n",
    "\n",
    "### Step 1: Run SAM on an image\n",
    "We\u2019ll start by running SAM directly on an image.  \n",
    "You\u2019ll see that SAM can segment all visible objects \u2014 but it doesn\u2019t know what any of them are.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Use a Vision-Language Model (VLM)\n",
    "Next, we\u2019ll introduce a **VLM** that can process both **images and text prompts**.  \n",
    "We\u2019ll ask it something like:\n",
    "> \u201cFind the phone in this image.\u201d\n",
    "\n",
    "The VLM will then **generate a bounding box** around the phone and return the dimensions of the bounding box.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Combine SAM and the VLM\n",
    "Finally, we\u2019ll use the **bounding box from the VLM as a prompt for SAM**.  \n",
    "SAM will then return a **high-precision segmentation mask** for just the phone!\n",
    "\n",
    "---\n",
    "\n",
    "# Learning Goal\n",
    "\n",
    "By the end of this notebook, you\u2019ll understand:\n",
    "- What SAM is and how it performs segmentation.\n",
    "- Why SAM alone isn\u2019t enough for language-based commands.\n",
    "- How VLMs can identify objects using text prompts.\n",
    "- How to use **VLM \u2192 bounding box \u2192 SAM** for natural-language-driven segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "## Note:\n",
    "Loading SAM and a VLM would be too costly for Deepnote so we (the TAs) have run SAM and the VLM offline. All functions in this notebook load the returned output from running SAM and a VLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk1-ZTx8sri6"
   },
   "source": [
    "\n",
    "# Step 1: Load an Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WR-acfOItyqu"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "Ftv7yg7Stu9n",
    "outputId": "b693d332-0583-41f8-f674-6621547075b0"
   },
   "outputs": [],
   "source": [
    "def load_raw_image() -> Image.Image:\n",
    "    try:\n",
    "        raw_url = \"https://raw.githubusercontent.com/RussTedrake/manipulation/refs/heads/master/book/figures/sam/raw_img.png\"\n",
    "        response = requests.get(raw_url)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def display_image(img: Image.Image | None, title: str = None) -> None:\n",
    "    if img is None:\n",
    "        print(\"Failed, no image to display.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "img = load_raw_image()\n",
    "display_image(img, title=\"Starting Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I6Ki4IIsx3Q"
   },
   "source": [
    "# Step 2: Run SAM on the Whole Image\n",
    "\n",
    "Now, let\u2019s apply **SAM** directly to the image *without any prompt*.  \n",
    "SAM will try to segment everything it sees \u2014 generating multiple object masks.\n",
    "\n",
    "*Note:* The TAs have already run SAM offline for you, (you're welcome). The function below returns the image from running SAM. Generally running SAM locally takes more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "P-o1f7x-uEFr",
    "outputId": "818d404b-5a29-4356-fef7-ef82f8595c8d"
   },
   "outputs": [],
   "source": [
    "def run_sam_no_prompt() -> Image.Image | None:\n",
    "    try:\n",
    "        print(\"Running SAM on input image ...\")\n",
    "        print(\"No prompt passed through. Generating a mask for the full image\")\n",
    "        time.sleep(1)\n",
    "        raw_url = \"https://raw.githubusercontent.com/RussTedrake/manipulation/refs/heads/master/book/figures/sam/output_segmentation_masks.png\"\n",
    "        response = requests.get(raw_url)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def display_image(img: Image.Image | None, title: str = None) -> None:\n",
    "    if img is None:\n",
    "        print(\"Failed, no image to display.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "img = run_sam_no_prompt()\n",
    "display_image(img, title=\"Initial Masked Image (no prompt)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZRBNCDNs1Bd"
   },
   "source": [
    "# Step 3: The Problem \u2014 SAM Can\u2019t Understand Language\n",
    "\n",
    "If we asked SAM to \"find the phone,\" it wouldn\u2019t know what that means.  \n",
    "SAM can segment *all* objects, but it doesn\u2019t know their **semantic meaning**. Futhermore, looking at the iPhone we see that it is oversegmented. The cameras and sticker have their own segmentation mask. We would like one segmentation mask for the iPhone.\n",
    "\n",
    "This is a fundamental limitation in pure vision models.\n",
    "\n",
    "Next, we\u2019ll see how a **Vision-Language Model (VLM)** helps solve this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YKUJCVGs5-R"
   },
   "source": [
    "# Step 4: Use a Vision-Language Model (VLM) to Find the Object\n",
    "\n",
    "Here, we\u2019ll use a **VLM (like GPT of Grounding DINO)** to process:\n",
    "- the **image**, and  \n",
    "- a **text prompt** like \u201cfind the phone.\u201d\n",
    "\n",
    "The VLM will interpret the text, detect which region matches the object, and return a **bounding box**.\n",
    "\n",
    "*Note:* Once again running a VLM would be too difficult to do in deepnote. The TAs have run the VLM offine. The function below returns the bounding box coordinates from the TAs VLM call.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "1irYUEF4vbFy",
    "outputId": "e8a0eaf9-a54c-40e4-ae2d-9209d78af180"
   },
   "outputs": [],
   "source": [
    "def call_vlm(prompt: str) -> list[int]:\n",
    "    print(\"Connecting to VLM...\")\n",
    "    print(f\"Using prompt: '{prompt}' \")\n",
    "\n",
    "    time.sleep(1)\n",
    "    return [42, 225, 214, 347]\n",
    "\n",
    "\n",
    "def display_bbox_on_phone(bbox_cords: list[int]) -> None:\n",
    "    raw_url = \"https://raw.githubusercontent.com/RussTedrake/manipulation/refs/heads/master/book/figures/sam/raw_img.png\"\n",
    "    response = requests.get(raw_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    x0, y0, x1, y1 = bbox_cords\n",
    "\n",
    "    w, h = x1 - x0, y1 - y0\n",
    "\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Bounding Box Around Phone\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "prompt = \"Return the coordinates for a bounding box around the phone in this image\"\n",
    "\n",
    "bbox_coords = call_vlm(prompt)\n",
    "print(\"\\n-------------\\n\")\n",
    "print(bbox_coords)\n",
    "print(\"Copy these coordinates to your clipboard. You will need them in the next cell.\")\n",
    "display_bbox_on_phone(bbox_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT2uZIEktEtO"
   },
   "source": [
    "# Step 5: Prompt SAM with the Bounding Box\n",
    "\n",
    "Now that the VLM has located the object (\u201cphone\u201d), we\u2019ll use that **bounding box as a prompt** for SAM.  \n",
    "SAM will then generate a **precise segmentation mask** for the object inside that box.\n",
    "\n",
    "#### TODO: Copy the bounding box generated in step 4 into the next cell. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "ZSQdTzRLxjRJ",
    "outputId": "c0d2ae7a-8ccb-40ee-e848-424e822c7e68"
   },
   "outputs": [],
   "source": [
    "def run_sam_bounding_box(bounding_box_coords: list[int]) -> Image.Image | None:\n",
    "    assert (\n",
    "        bounding_box_coords is not None\n",
    "    ), \"Bounding box coordinates must be provided. Copy them from the cell above\"\n",
    "    try:\n",
    "        print(\"Running SAM on input image ...\")\n",
    "        print(\"Using_bounding_box_prompt. Generating a mask for boxed region\")\n",
    "        time.sleep(1)\n",
    "        raw_url = sam_prompt = (\n",
    "            \"https://raw.githubusercontent.com/RussTedrake/manipulation/refs/heads/master/book/figures/sam/mask_with_bounding_box_overlay.png\"\n",
    "        )\n",
    "\n",
    "        response = requests.get(raw_url)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def display_image(img: Image.Image, title: str = None):\n",
    "    if img is None:\n",
    "        print(\"Failed, no image to display.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# TODO: copy bounding box coords from above\n",
    "bbox_coords = None\n",
    "if bbox_coords is not None:\n",
    "    img = run_sam_bounding_box(bbox_coords)\n",
    "    display_image(img, title=\"Segmentation Mask with Bounding Box Prompting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmMxgS1CyY8g"
   },
   "source": [
    "## Great! Now we have a clear segmentation of the phone that we can use for future manipulation and planning tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM61D_yMtNK0"
   },
   "source": [
    "# Summary and Takeaways\n",
    "\n",
    "- SAM sees *everything*,  \n",
    "- VLM understands *what things are*,  \n",
    "- Together, they allow systems to *see, understand, and act* based on human language.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}