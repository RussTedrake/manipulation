{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e92546c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Notebook 7: Pick And Place With Geometry\n",
    "\n",
    "In the previous notebooks, we designed various trajectories for the robot arm to follow, created a `DifferentialIKSystem`, used both pseudoinverse and optimization-based controllers to move the robot arm and used ICP to register point-clouds for object meshes from the scene cameras. In this notebook, we will combine these tools together to build an entire pick and place system based on the previous psets that pick up your initials.\n",
    "\n",
    "*Learning Objectives:*\n",
    "Build an end to end system that  \n",
    "1. Sets up a scenario with arbitrary meshes\n",
    "2. Uses scene cameras and ICP to regsiters the geometry of the objects\n",
    "3. Uses a Differential-IK controller to do pick and place.\n",
    "\n",
    "*What You'll Build:* An end to end pick and place system, featuring your intiials from Notebook 2. \n",
    "\n",
    "*Refence*: TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import trimesh\n",
    "from pydrake.all import (\n",
    "    AddFrameTriadIllustration,\n",
    "    BasicVector,\n",
    "    Concatenate,\n",
    "    Context,\n",
    "    Diagram,\n",
    "    DiagramBuilder,\n",
    "    Integrator,\n",
    "    JacobianWrtVariable,\n",
    "    LeafSystem,\n",
    "    MultibodyPlant,\n",
    "    PiecewisePolynomial,\n",
    "    PiecewisePose,\n",
    "    PointCloud,\n",
    "    Rgba,\n",
    "    RigidTransform,\n",
    "    RobotDiagram,\n",
    "    RollPitchYaw,\n",
    "    RotationMatrix,\n",
    "    Simulator,\n",
    "    StartMeshcat,\n",
    "    Trajectory,\n",
    "    TrajectorySource,\n",
    ")\n",
    "\n",
    "from manipulation.exercises.grader import Grader\n",
    "from manipulation.icp import IterativeClosestPoint\n",
    "from manipulation.letter_generation import create_sdf_asset_from_letter\n",
    "from manipulation.meshcat_utils import AddMeshcatTriad\n",
    "from manipulation.station import (\n",
    "    AddPointClouds,\n",
    "    LoadScenario,\n",
    "    MakeHardwareStation,\n",
    "    RobotDiagram,\n",
    ")\n",
    "from manipulation.utils import RenderDiagram, running_as_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbfb68f",
   "metadata": {},
   "source": [
    "# Meshcat Visualization\n",
    "\n",
    "As always, start by visualizing in Meshcat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121dacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start meshcat for visualization\n",
    "meshcat = StartMeshcat()\n",
    "print(\"Click the link above to open Meshcat in your browser!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db42b9",
   "metadata": {},
   "source": [
    "# Part 1: Setting Up the Scenario\n",
    "\n",
    "In Pset1 Notebook 3, you created a scenario file for custom objects and used the `MakeHardwareStation` function and `LoadScenario` functions to create a simulation with your initials.\n",
    "\n",
    "In this section, we'll repeat that setup so we can get started with our simulation. Unlike in that notebook, however, this time we will also add a set of scene cameras so we can also sense object locations. We'll make use of the `LoadScenario` function which will abstract away setting up the systems for all the cameras and the driver logic. In this section, we'll add onto this and create a more complicated scenario using all of the different types of functionalities of a scenario:\n",
    "* Directives for all the robot arms, tables, and cameras\n",
    "* Model drivers for all the robot arms so they can actually be actuated\n",
    "* Cameras with corresponding configs. \n",
    "\n",
    "Because our simulation now consists of a lot of moving parts including the directives for both the robot arms and cameras, we will modularize the components into two parts: the directives and drivers/cameras. Then, simply running `LoadScenario` to get a `Scenario` config object and then running `MakeHardwareStation` will generate all the necessary systems for all the cameras which will allow us to generate the necessary point cloud geometry. \n",
    "\n",
    "First, we generate the initials and table assets again. For simplicity, in this notebook we will ask that you only generate one of your intiials. In order to use the depth camera sensors to get the point clouds, we need to make sure our meshes also have all the normal information added (in real life all of this information is just available in the scene but in simulation these are properties that must be made available). Fortunately, this can be done by simply adding the`include_normal=True` argument to the original `create_sdf_assets_from_letter` function. Let's do that below:\n",
    "\n",
    "**YOUR TASK**: Regenerate the letters with normals, create a station with the robot, cameras, and depth sensors, and add the corresponding systems.\n",
    "\n",
    "**Key Concepts**: Using directives to append scenarios and adding point clouds to the system with `AddPointClouds` and `DepthImageToPointClouds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A note: as an additional argument add `include_normals=True` to the `create_sdf_asset_from_letter` function.\n",
    "# We also recommend you use a slightly smaller `letter_height_meters=0.15` and a rather high friction coefficient\n",
    "# (we use mu_static = 1.17, mu_dynamic = 1.0 - roughly equal to the friction of rubber on granite) to make the\n",
    "# letters easier to pick up. We'll study more complicated grasping strategies in future notebooks, but this is not\n",
    "# the priority of this chapter so we'll cheat.\n",
    "\n",
    "output_dir = Path(\"assets/\")\n",
    "\n",
    "# TODO: replace ADD_YOUR_INITIAL_HERE with your initial.\n",
    "your_initial = \"ADD_YOUR_INITIAL_HERE\"\n",
    "\n",
    "# TODO: generate the SDF file for your intial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7cb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Paste your table_sdf = ... code from Pset1 Notebook 2 here\n",
    "\n",
    "# TODO: Write the table SDF to assets/table.sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f065407",
   "metadata": {},
   "source": [
    "We'll now create a scenario with a Kuka IIWA arm, a table, and your initials along with a set of scene cameras. Because our scenario is becoming quite big, we'll first split up the scenario directives. We'll then load the directives and include the scene cameras and the drivers later in the scenarip. As a reference, look at this [example](https://github.com/RussTedrake/manipulation/blob/3b74947270c02c7202cc33197d1e41d2cf34dea0/manipulation/models/clutter.scenarios.yaml) scenario file for an example of how to do so.\n",
    "\n",
    "Once we have the correct scenario file, we can simply run `LoadScenario` and this will generate all of the camera systems. Unlike in the last problem set, we recommend adding around 3 scene cameras (the default settings in the example should work), but feel free to play around with the exact geometry of the cameras. \n",
    "\n",
    "Also, play around a bit with the exact starting position (the `X_PC` of the `iiwa_link_0` relative to the world frame). Make it start in a reasonable position relative to the letters, although it doesn't have to be perfect as we use now use geometric sensing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9480a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the directives for the bimanual IIWA arms, table, and initials\n",
    "\n",
    "\n",
    "def generate_bimanual_IIWA14_with_table_and_initials_and_assets_directives_file() -> (\n",
    "    tuple[Diagram, RobotDiagram]\n",
    "):\n",
    "    table_sdf = f\"{Path.cwd()}/assets/table.sdf\"\n",
    "    letter_sdf = f\"{Path.cwd()}/assets/{your_initial}_model/{your_initial}.sdf\"\n",
    "\n",
    "    directives_yaml = f\"\"\"directives:\n",
    "- add_model:\n",
    "    name: iiwa\n",
    "    file: package://drake_models/iiwa_description/sdf/iiwa7_no_collision.sdf\n",
    "    default_joint_positions:\n",
    "        iiwa_joint_1: [-1.57]\n",
    "        iiwa_joint_2: [0.1]\n",
    "        iiwa_joint_3: [0]\n",
    "        iiwa_joint_4: [-1.2]\n",
    "        iiwa_joint_5: [0]\n",
    "        iiwa_joint_6: [ 1.6]\n",
    "        iiwa_joint_7: [0]\n",
    "- add_weld:\n",
    "    parent: world\n",
    "    child: iiwa::iiwa_link_0\n",
    "    X_PC:\n",
    "        translation: [0, -0.5, 0]\n",
    "        rotation: !Rpy {{ deg: [0, 0, 180] }}\n",
    "- add_model:\n",
    "    name: wsg\n",
    "    file: package://manipulation/hydro/schunk_wsg_50_with_tip.sdf\n",
    "- add_weld:\n",
    "    parent: iiwa::iiwa_link_7\n",
    "    child: wsg::body\n",
    "    X_PC:\n",
    "        translation: [0, 0, 0.09]\n",
    "        rotation: !Rpy {{ deg: [90, 0, 90]}}\n",
    "- add_model:\n",
    "    name: table\n",
    "    file: file://{table_sdf}\n",
    "- add_weld:\n",
    "    parent: world\n",
    "    child: table::table_link\n",
    "    X_PC:\n",
    "        translation: [0.0, 0.0, -0.05]\n",
    "        rotation: !Rpy {{ deg: [0, 0, -90] }}\n",
    "- add_model:\n",
    "    name: {your_initial}_letter\n",
    "    file: file://{letter_sdf}\n",
    "    default_free_body_pose:\n",
    "        {your_initial}_body_link:\n",
    "            translation: [-0.35, 0, 0]\n",
    "            rotation: !Rpy {{ deg: [0, 0, 0] }}\n",
    "\"\"\"\n",
    "    os.makedirs(\"directives\", exist_ok=True)\n",
    "\n",
    "    # TODO: save the directives as a file. Instead of saving the directives under filename \"scenario.yaml\", save it under filename\n",
    "    # `bimanual_IIWA14_with_table_and_initials_and_assets.dmd.yaml` in the `directives` directory.\n",
    "\n",
    "\n",
    "generate_bimanual_IIWA14_with_table_and_initials_and_assets_directives_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff4453",
   "metadata": {},
   "source": [
    "We'll now add the camera directives into a seperate directive and then append them into a single scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86806c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_camera_directives() -> None:\n",
    "    camera_directives_yaml = \"\"\"\n",
    "directives:\n",
    "- add_frame:\n",
    "    name: camera0_origin\n",
    "    X_PF:\n",
    "        base_frame: world\n",
    "        rotation: !Rpy { deg: [-120.0, 0.0, 180.0]}\n",
    "        translation: [0, 0.8, 0.5]\n",
    "\n",
    "- add_model:\n",
    "    name: camera0\n",
    "    file: package://manipulation/camera_box.sdf\n",
    "\n",
    "- add_weld:\n",
    "    parent: camera0_origin\n",
    "    child: camera0::base\n",
    "\n",
    "- add_frame:\n",
    "    name: camera1_origin\n",
    "    X_PF:\n",
    "        base_frame: world\n",
    "        rotation: !Rpy { deg: [-125, 0.0, 90.0]}\n",
    "        translation: [0.8, 0.1, 0.5]\n",
    "\n",
    "- add_model:\n",
    "    name: camera1\n",
    "    file: package://manipulation/camera_box.sdf\n",
    "\n",
    "- add_weld:\n",
    "    parent: camera1_origin\n",
    "    child: camera1::base\n",
    "\n",
    "- add_frame:\n",
    "    name: camera2_origin\n",
    "    X_PF:\n",
    "        base_frame: world\n",
    "        rotation: !Rpy { deg: [-120.0, 0.0, -90.0]}\n",
    "        translation: [-0.8, 0.1, 0.5]\n",
    "\n",
    "- add_model:\n",
    "    name: camera2\n",
    "    file: package://manipulation/camera_box.sdf\n",
    "\n",
    "- add_weld:\n",
    "    parent: camera2_origin\n",
    "    child: camera2::base\n",
    "\"\"\"\n",
    "    with open(\"directives/camera_directives.dmd.yaml\", \"w\") as f:\n",
    "        f.write(camera_directives_yaml)\n",
    "\n",
    "\n",
    "create_camera_directives()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b35af3f",
   "metadata": {},
   "source": [
    "Next, we'll combine the directives, camera configs, and drivers into a single scenario file. Look back at the [clutter scenario](https://github.com/RussTedrake/manipulation/blob/3b74947270c02c7202cc33197d1e41d2cf34dea0/manipulation/models/clutter.scenarios.yaml) example to see how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bimanual_IIWA14_with_table_and_initials_and_assets_and_cameras_scenario() -> (\n",
    "    None\n",
    "):\n",
    "    # TODO: create a scenario yaml with the directives added with `add_directives`\n",
    "\n",
    "    # TODO: add the camera configs and iiwa drivers with `add_cameras` and `add_iiwa_drivers`\n",
    "\n",
    "    os.makedirs(\"scenarios\", exist_ok=True)\n",
    "\n",
    "    # TODO: save the scenario to a file\n",
    "\n",
    "\n",
    "create_bimanual_IIWA14_with_table_and_initials_and_assets_and_cameras_scenario()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342882aa",
   "metadata": {},
   "source": [
    "Now that we have the scenario file, we'll load it up into a DiagramBuilder using `MakeHardwareStation`. In order to get the point clouds for the meshes we will need to add the neccesary systems to the diagram. As in the previous notebook, we make use of the `AddPointClouds`function in `manipulation.station` which under the hood adds a `DepthImageToPointCloud` system to each camera (and connects it to the corresponding ports) in order to extract the corresponding point clouds. We will then need to export the output ports for this system to the output of the diagram allowing for these point clouds to be accesed for when we do ICP later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8473abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bimanual_IIWA14_with_table_and_initials_and_assets_and_cameras() -> (\n",
    "    Tuple[DiagramBuilder, RobotDiagram]\n",
    "):\n",
    "    # TODO: Load the scenario created above into a Scenario object\n",
    "\n",
    "    # TODO: Create HardwareStation with the scenario and meshcat\n",
    "\n",
    "    # TODO: Make a DiagramBuilder, add the station, and build the diagram\n",
    "\n",
    "    # TODO: Add the point clouds to the diagram with AddPointClouds\n",
    "\n",
    "    # TODO: export the point cloud outputs to the builder\n",
    "\n",
    "    # TODO: Return the builder AND the station (notice that here we will need both)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d8c579",
   "metadata": {},
   "source": [
    "Great! We now have a scenario with our robot, assets, and the cameras. Build them up and render a diagram fromo the builder below to check for any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder, station = (\n",
    "    create_bimanual_IIWA14_with_table_and_initials_and_assets_and_cameras()\n",
    ")\n",
    "\n",
    "# in order to debug, we will build the diagram once here.\n",
    "diagram = builder.Build()\n",
    "\n",
    "# visualize the diagram\n",
    "RenderDiagram(diagram, max_depth=1)\n",
    "\n",
    "# publish the diagram with some default context\n",
    "context = diagram.CreateDefaultContext()\n",
    "diagram.ForcedPublish(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905ddf8",
   "metadata": {},
   "source": [
    "# Part 2. Registering the Point Clouds With ICP\n",
    "\n",
    "In the pose estimation ICP notebook, we cropped out the objects of interest and used the depth cameras to get a single camera's point cloud and then used ICP to register our object geometries. In this notebook, we'll take a slightly more realistic example: rather than using a single point cloud, we will make use of point clouds from multiple different cameras by merging them through concatenation (as a follow up, in next week's problem set you will use RANSAC to remove outliers in the point cloud - in this notebook we simply merge them and call it a day). \n",
    "\n",
    "Finally, we'll make use of ICP to register the point cloud with our known geometries. Unlike in previous notebooks, rather than coding ICP from scratch, we'll make use of the `IterativeClosestPoint` function in `manipulation.icp`. \n",
    "\n",
    "**Your Task**: segment out the object of interest and get the point clouds, merge the point clouds with concatenation, and then use ICP to register point cloud geometries.\n",
    "\n",
    "**Key Concepts**: Cropping out the point clouds, registering the point cloud geometries using the ICP function in `manipulation.icp`. \n",
    "\n",
    "**References**: [Point Cloud Processing Notebook](https://github.com/RussTedrake/manipulation/blob/c9f8d668de91241d170e3b6d692c4299e6f5e055/book/clutter/point_cloud_processing.ipynb#L45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bc81f",
   "metadata": {},
   "source": [
    "## Getting the Point Cloud of the Model\n",
    "\n",
    "Before running ICP, we need a point cloud of the *model* to compare against. To do this, we'll load into a `PointCloud` object the obj files corresponding to the letters. \n",
    "\n",
    "We'll make use of the [trimesh](https://trimesh.org/) library to do this. Because our obj file contains the full 3d geometry (rather than a set of points for a point cloud) we will need to load the obj file as a mesh and then sample points from it to get a point cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f37f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLE_POINTS = 1500\n",
    "\n",
    "# TODO: load your intitials with trimesh as a mesh\n",
    "\n",
    "# TODO: sample N_SAMPLE_POINTS from the mesh and then turn those points into a numpy array (you might need to transpose)\n",
    "\n",
    "# TODO: create a `PointCloud` object from the numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e011c",
   "metadata": {},
   "source": [
    "## Cropping Out the Point Clouds\n",
    "\n",
    "Next, we'll get the camera point clouds. We'll start by cropping out the objects of interest within each camera. \n",
    "\n",
    "Start by visualizing the rgb and depth outputs of each of the rgbd sensors to make sure your camera is in the right position. If it's not, edit the camera rotation and/or translation so they roughly align with your initials. It doesn't have to be perfect as we'll segment out the objects later: the objects just have to lie in the frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the following cell to visualize the rgb outputs of each of the cameras\n",
    "cameras = [\"camera0\", \"camera1\", \"camera2\"]\n",
    "station_context = diagram.GetSubsystemContext(station, context)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, len(cameras), figsize=(5 * len(cameras), 4), constrained_layout=True\n",
    ")\n",
    "for ax, cam in zip(axes, cameras):\n",
    "    img = station.GetOutputPort(f\"{cam}.rgb_image\").Eval(station_context)\n",
    "    arr = np.array(img.data, copy=False).reshape(img.height(), img.width(), -1)\n",
    "    im = ax.imshow(arr)\n",
    "    ax.set_title(f\"{cam} rgb image\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1020dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the following cell to visualize the depth outputs of each of the cameras\n",
    "fig, axes = plt.subplots(\n",
    "    1, len(cameras), figsize=(5 * len(cameras), 4), constrained_layout=True\n",
    ")\n",
    "for ax, cam in zip(axes, cameras):\n",
    "    img = station.GetOutputPort(f\"{cam}.depth_image\").Eval(station_context)\n",
    "    depth_img = np.array(img.data, copy=False).reshape(img.height(), img.width(), -1)\n",
    "    depth_img = np.ma.masked_invalid(depth_img)\n",
    "    img = ax.imshow(depth_img, cmap=\"magma\")\n",
    "    ax.set_title(f\"{cam} depth image\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e322381",
   "metadata": {},
   "source": [
    "Next, we'll get the point clouds from each of the three cameras and simply crop out the desired regions (the regions that contain the letters) based on position. Here, we're going to cheat a little bit and use the reference positions of the letters based on the cheat ports in the system to save you some time, but in the real world you should either manually change the positions of the bounding boxes. We'll also remove the table point cloud as the point clouds from the table are not of interest. \n",
    "\n",
    "For the final project, feel free to use more interesting strategies for segmentation including color-based ones, outlier detection algorithms, end-to-end vision-based segmentation algorithms, and so forth. \n",
    "\n",
    "Use the [Mustard Bottle Point Cloud Processing](https://github.com/RussTedrake/manipulation/blob/c9f8d668de91241d170e3b6d692c4299e6f5e055/book/clutter/point_cloud_processing.ipynb#L75) and [Clutter Clearing](https://github.com/RussTedrake/manipulation/blob/c9f8d668de91241d170e3b6d692c4299e6f5e055/book/clutter/clutter_clearing.ipynb#L4) notebooks as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b73511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the actual relative positions of each of the letters to use as a reference\n",
    "# for cropping the point clouds.\n",
    "plant = station.plant()\n",
    "plant_context = diagram.GetSubsystemContext(plant, context)\n",
    "\n",
    "world_frame = plant.world_frame()\n",
    "\n",
    "# TODO: change the following lines to match your initial\n",
    "model_B = plant.GetModelInstanceByName(\"B_letter\")\n",
    "frame_B = plant.GetFrameByName(\"B_body_link\", model_instance=model_B)\n",
    "X_PC_B = plant.CalcRelativeTransform(plant_context, world_frame, frame_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d0d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get the point clouds from each of the cameras. For now, don't worry too much about\n",
    "# removing the table points as we'll do that later.\n",
    "\n",
    "# TODO: crop out each of the letters from each of the cameras\n",
    "# HINT: A simple way to do this is to use a simple Crop based on the X_PC_{letter} transforms given above.\n",
    "#       Add a small offset based on the size of your letters to make the whole letter fit in the crop.\n",
    "#       We add a visualization below to help you visualize the cropped point clouds.\n",
    "\n",
    "B_letter_lower = None  # TODO\n",
    "B_letter_upper = None  # TODO\n",
    "\n",
    "camera0_B_point_cloud = None  # TODO\n",
    "camera1_B_point_cloud = None  # TODO\n",
    "camera2_B_point_cloud = None  # TODO\n",
    "\n",
    "# This will visualize the bounding boxes for each of the letters in the point clouds\n",
    "# Use it to adjust your bounding boxes to fit the letters.\n",
    "if B_letter_lower is not None and B_letter_upper is not None:\n",
    "    meshcat.SetLineSegments(\n",
    "        \"bounding_line\",\n",
    "        np.array(B_letter_lower).T,\n",
    "        np.array(B_letter_upper).T,\n",
    "        1.0,\n",
    "        Rgba(0, 0, 0),\n",
    "    )\n",
    "\n",
    "# TODO: concatenate the point clouds\n",
    "\n",
    "# TODO: downsample the point clouds\n",
    "# HINT: use the `VoxelizedDownSample` method for PointCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94a5d1",
   "metadata": {},
   "source": [
    "Next, we'll subtract away the point clouds for all the points in the table. We'll cheat a little bit and use the fact that all points in the table lie below around z=0.01 so we'll remove all of those points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement a function that removes all points in the point cloud that lie below z=0\n",
    "\n",
    "\n",
    "def remove_table_points(point_cloud: PointCloud) -> PointCloud:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "# TODO: remove the table points from the concatenated point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07517a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement a function that removes all points in the point cloud that lie below z=0\n",
    "def remove_table_points(point_cloud: PointCloud) -> PointCloud:\n",
    "    return point_cloud.Crop(\n",
    "        lower_xyz=np.array([-np.inf, -np.inf, 0.01]),\n",
    "        upper_xyz=np.array([np.inf, np.inf, np.inf]),\n",
    "    )\n",
    "\n",
    "\n",
    "# TODO: remove the table points from the concatenated point clouds\n",
    "B_point_cloud = remove_table_points(B_point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the concatenated point clouds\n",
    "meshcat.SetObject(\"B_point_cloud\", B_point_cloud, point_size=0.05, rgba=Rgba(1, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd220d",
   "metadata": {},
   "source": [
    "If everything is done correctly, you should be able to see point clouds for your intials that are created from your camera like below:\n",
    "\n",
    "![ps3_p4](https://github.com/brianjsl/manipulation/blob/pset3/book/figures/exercises/geometry_pick_and_place_point_clouds.png?raw=true)\n",
    "\n",
    "NOTE: the things you should be visualizing should be `{letter}_source_cloud` NOT the `{letter}_model_source_cloud`s we created for reference for ICP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ef69d",
   "metadata": {},
   "source": [
    "## Registering Point Cloud Geometry with ICP\n",
    "\n",
    "Now that we have the point clouds make use of the built in `IterativeClosestPoint` function to register the point cloud geometries. Use the [Pose Estimation ICP](https://github.com/RussTedrake/manipulation/blob/c9f8d668de91241d170e3b6d692c4299e6f5e055/book/pose/exercises/pose_estimation_icp.ipynb#L4) notebook as a reference. You should end up with a pose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a6b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 25 if running_as_notebook else 2\n",
    "\n",
    "# TODO: set initial guesses for each of the letters along with the maximum number of iterations\n",
    "# These can be rough guesses as ICP will do the bulk of the work of aligning the point clouds.\n",
    "\n",
    "# TODO: convert both the model and generated point clouds to numpy arrays to pass into the ICP function\n",
    "\n",
    "# TODO: register the point clouds with the model point clouds using ICP for each of the letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a05be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the error in the registration for each of the letters below:\n",
    "# if it has converged, all errors should be close to zero\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "error_B = B_X_Ohat.inverse().multiply(X_PC_B)\n",
    "\n",
    "rpy = RollPitchYaw(error_B.rotation()).vector()\n",
    "xyz = error_B.translation()\n",
    "print(f\"B error: rpy: {rpy}, xyz: {xyz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4385bd",
   "metadata": {},
   "source": [
    "# Part 3: Pick and Place with Registered Geometries\n",
    "\n",
    "Phew! Now that we finally have all the geometry figured out, we can start doing pick and place with the initials. For this part, we'll mainly just be reusing the code from the previous pset with some minor edits. We'll be using a pseudoinverse controller for the pick and place, but for practice feel free to use something more complicated like an optimization-based controller which might come useful for the final project. \n",
    "\n",
    "We'll follow the same general pattern as the previous notebook: this time, you will be tasked with getting at least one of your initials upright (so that the z axis of the object and the z axis of the world are parallel). This time we'll add a few more steps so the general trajectory will follow the pattern initial -> prepick -> pick -> preplace -> place -> postplace -> initial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify the functions below to work with your letters. This will require quite a bit of trial and error.\n",
    "\n",
    "\n",
    "def design_grasp_pose(X_WO: RigidTransform) -> Tuple[RigidTransform, RigidTransform]:\n",
    "    R_OG = (\n",
    "        RollPitchYaw(0, 0, np.pi).ToRotationMatrix()\n",
    "        @ RollPitchYaw(-np.pi / 2, 0, 0).ToRotationMatrix()\n",
    "    )\n",
    "    p_OG = [0.07, 0.08, 0.12]\n",
    "    X_OG = RigidTransform(R_OG, p_OG)\n",
    "    X_WG = X_WO.multiply(X_OG)\n",
    "    return X_OG, X_WG\n",
    "\n",
    "\n",
    "def design_pregrasp_pose(\n",
    "    X_WG: RigidTransform,\n",
    ") -> Tuple[RigidTransform, RigidTransform, RigidTransform]:\n",
    "    X_GGApproach = RigidTransform([0.0, -0.2, 0.0])\n",
    "    X_WGApproach = X_WG @ X_GGApproach\n",
    "    return X_WGApproach\n",
    "\n",
    "\n",
    "def design_pregoal_pose(\n",
    "    X_WG: RigidTransform,\n",
    ") -> Tuple[RigidTransform, RigidTransform, RigidTransform]:\n",
    "    X_GGApproach = RigidTransform([0.0, 0.0, -0.2])\n",
    "    X_WGApproach = X_WG @ X_GGApproach\n",
    "    return X_WGApproach\n",
    "\n",
    "\n",
    "# The goal poses have been modified to include the third initial.\n",
    "def design_goal_poses(\n",
    "    X_WO: RigidTransform, X_OG: RigidTransform\n",
    ") -> tuple[RigidTransform, RigidTransform, RigidTransform]:\n",
    "    X_WOgoal = X_WO @ RigidTransform(\n",
    "        R=RotationMatrix.MakeXRotation(np.pi / 2), p=np.array([-0.1, 0.2, 0.03])\n",
    "    )\n",
    "    X_WGgoal = X_WOgoal @ X_OG\n",
    "    return X_WGgoal\n",
    "\n",
    "\n",
    "def design_postgoal_pose(\n",
    "    X_WG: RigidTransform,\n",
    ") -> Tuple[RigidTransform, RigidTransform, RigidTransform]:\n",
    "    X_GGApproach = RigidTransform([0.0, 0.0, -0.2])\n",
    "    X_WGApproach = X_WG @ X_GGApproach\n",
    "    return X_WGApproach\n",
    "\n",
    "\n",
    "def make_trajectory(\n",
    "    X_Gs: list[RigidTransform], finger_values: np.ndarray, sample_times: list[float]\n",
    ") -> tuple[Trajectory, PiecewisePolynomial]:\n",
    "    robot_position_trajectory = PiecewisePose.MakeLinear(sample_times, X_Gs)\n",
    "    robot_velocity_trajectory = robot_position_trajectory.MakeDerivative()\n",
    "    traj_wsg_command = PiecewisePolynomial.FirstOrderHold(sample_times, finger_values)\n",
    "    return robot_velocity_trajectory, traj_wsg_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: copy over the pseudoinverse controller from the previous pset below.\n",
    "# OPTIONAL: if you want to you can try designing a more complicated controller for the pick and place.\n",
    "\n",
    "\n",
    "class PseudoInverseController(LeafSystem):\n",
    "    def __init__(self, plant: MultibodyPlant):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def CalcOutput(self, context: Context, output: BasicVector):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a2cfb",
   "metadata": {},
   "source": [
    "Now that we have our controller along with the poses of each of the objects, we can finally finish our pick and place. This section will also mainly be copying over code from the previous pset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e971166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will rebuild the diagram in order to add the controller and integrator systems we need.\n",
    "builder, station = (\n",
    "    create_bimanual_IIWA14_with_table_and_initials_and_assets_and_cameras()\n",
    ")\n",
    "plant = station.GetSubsystemByName(\"plant\")\n",
    "\n",
    "station_context = station.CreateDefaultContext()\n",
    "plant_context = plant.GetMyContextFromRoot(station_context)\n",
    "\n",
    "# get initial poses of gripper and objects\n",
    "X_WGinitial = plant.EvalBodyPoseInWorld(plant_context, plant.GetBodyByName(\"body\"))\n",
    "\n",
    "# TODO: copy over the poses registered from ICP above\n",
    "X_WOinitial = RigidTransform()  # TODO\n",
    "\n",
    "# Build trajectory keyframes\n",
    "X_OG, X_WGpick = design_grasp_pose(X_WOinitial)\n",
    "X_WGprepick = design_pregrasp_pose(X_WGpick)\n",
    "X_WGgoal = design_goal_poses(X_WOinitial, X_OG)\n",
    "X_WGpregoal = design_pregoal_pose(X_WGgoal)\n",
    "X_WGpostgoal = design_postgoal_pose(X_WGgoal)\n",
    "\n",
    "# constants for finger distances when the gripper is opened or closed\n",
    "opened = 0.107\n",
    "closed = 0.0\n",
    "\n",
    "# list of keyframes, formatted as (gripper poses, finger states)\n",
    "# for each object the robot starts in its default pose with its gripper open\n",
    "# then it goes to the prepick pose, the pick pose, closes the gripper, and then goes\n",
    "# to the place pose\n",
    "keyframes = [\n",
    "    (\"X_WGinitial\", X_WGinitial, opened),\n",
    "    (\"X_WGprepick\", X_WGprepick, opened),\n",
    "    (\"X_WGpick\", X_WGpick, opened),\n",
    "    (\"X_WGpick\", X_WGpick, closed),\n",
    "    (\"X_WGpregoal\", X_WGpregoal, closed),\n",
    "    (\"X_WGgoal\", X_WGgoal, closed),\n",
    "    (\"X_WGgoal\", X_WGgoal, opened),\n",
    "    (\"X_WGpostgoal\", X_WGpostgoal, opened),\n",
    "    (\"X_WGinitial\", X_WGinitial, opened),\n",
    "]\n",
    "\n",
    "# TODO: copy over your work from the previous pset\n",
    "gripper_poses = [keyframe[1] for keyframe in keyframes]\n",
    "finger_states = np.asarray([keyframe[2] for keyframe in keyframes]).reshape(1, -1)\n",
    "sample_times = [3 * i for i in range(len(gripper_poses))]\n",
    "traj_V_G, traj_wsg_command = make_trajectory(gripper_poses, finger_states, sample_times)\n",
    "\n",
    "# V_G_source defines a trajectory over gripper velocities. Add it to the system.\n",
    "V_G_source = builder.AddSystem(TrajectorySource(traj_V_G))\n",
    "# Add the DiffIK controller we just defined to the system\n",
    "controller = builder.AddSystem(PseudoInverseController(plant))\n",
    "# The HardwareStation expects robot commands in terms of joint angles.\n",
    "# We define the `integrator` system to map from joint_velocities to joint_angles.\n",
    "integrator = builder.AddSystem(Integrator(7))\n",
    "# wsg_source defines a trajectory of finger positions. Add it to the system.\n",
    "wsg_source = builder.AddSystem(TrajectorySource(traj_wsg_command))\n",
    "\n",
    "# TODO: connect the joint velocity source to the pseudoinverse controller\n",
    "# TODO: connect the controller to integrator to get joint angle commands\n",
    "# TODO: connect the joint angles computed by the integrateor to the iiwa.position port on the manipulation station\n",
    "# TODO: connect the \"iiwa.position_measured\" port on the station back to the relevant input port on the controller\n",
    "# TODO: connect the wsg_source to the \"wsg.position\" input port of the station\n",
    "\n",
    "# visualize axes (useful for debugging)\n",
    "scenegraph = station.GetSubsystemByName(\"scene_graph\")\n",
    "AddFrameTriadIllustration(\n",
    "    scene_graph=scenegraph,\n",
    "    body=plant.GetBodyByName(f\"{your_initial}_body_link\"),\n",
    "    length=0.1,\n",
    ")\n",
    "AddFrameTriadIllustration(\n",
    "    scene_graph=scenegraph, body=plant.GetBodyByName(\"body\"), length=0.1\n",
    ")\n",
    "\n",
    "diagram = builder.Build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simulator\n",
    "\n",
    "simulator = Simulator(diagram)\n",
    "context = simulator.get_mutable_context()\n",
    "station_context = station.GetMyContextFromRoot(context)\n",
    "integrator.set_integral_value(\n",
    "    integrator.GetMyContextFromRoot(context),\n",
    "    plant.GetPositions(\n",
    "        plant.GetMyContextFromRoot(context),\n",
    "        plant.GetModelInstanceByName(\"iiwa\"),\n",
    "    ),\n",
    ")\n",
    "diagram.ForcedPublish(context)\n",
    "print(f\"sanity check, simulation will run for {traj_V_G.end_time()} seconds\")\n",
    "\n",
    "# run simulation!\n",
    "meshcat.StartRecording()\n",
    "if running_as_notebook:\n",
    "    simulator.set_target_realtime_rate(1.0)\n",
    "simulator.AdvanceTo(traj_V_G.end_time())\n",
    "meshcat.StopRecording()\n",
    "meshcat.PublishRecording()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc4df58",
   "metadata": {},
   "source": [
    "Congratulations! If you've done everything right you should now have a fully end to end system that finds the geometry of the objects, registers the geometries with ICP, and picks them and places them upright. The final result should look something like below: \n",
    "\n",
    "![pick-and-place-geometry-initials-upright](https://github.com/brianjsl/manipulation/blob/pset3/book/figures/exercises/pick-and-place-geometry-initials-upright.png?raw=true)\n",
    "\n",
    "As in the previous pset, if the robot fails to grasp a letter, or if the letter seems to be slipping inside the robot's fingers, experiment with different grasp poses. Different letters may call for higher or lower grasps. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88dc59",
   "metadata": {},
   "source": [
    "BONUS: As a challenge, try getting all three letters upright without knocking the other letters over! We'll cover ways to do this (motion planning) in later chapters.\n",
    "\n",
    "**Once you have everything working, take a video and upload it to gradescope!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "64210",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}